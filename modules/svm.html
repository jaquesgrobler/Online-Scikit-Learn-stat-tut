

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>3.2. Support Vector Machines &mdash; scikit-learn 0.11-git documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.11-git',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.11-git documentation" href="../index.html" />
    <link rel="up" title="3. Supervised learning" href="../supervised_learning.html" />
    <link rel="next" title="3.3. Stochastic Gradient Descent" href="sgd.html" />
    <link rel="prev" title="3.1. Generalized Linear Models" href="linear_model.html" />
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="../install.html">Download</a></li>
            <li><a href="../support.html">Support</a></li>
            <li><a href="../user_guide.html">User Guide</a></li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            <li><a href="classes.html">Reference</a></li>
       </ul>

<div class="search_form">

<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>

</div>
          </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

    <div class="sphinxsidebar">
	<div class="rel">
	
	<!-- rellinks[1:] is an ugly hack to avoid link to module
	    index  -->
	<div class="rellink">
	<a href="linear_model.html" title="3.1. Generalized Linear Models"
	    accesskey="P">Previous
	    <br>
	    <span class="smallrellink">
	    3.1. Generalized...
	    </span>
	    <span class="hiddenrellink">
	    3.1. Generalized Linear Models
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="sgd.html" title="3.3. Stochastic Gradient Descent"
	    accesskey="N">Next
	    <br>
	    <span class="smallrellink">
	    3.3. Stochastic ...
	    </span>
	    <span class="hiddenrellink">
	    3.3. Stochastic Gradient Descent
	    </span>
	    
	    </a>
	</div>
	<!-- Ad a link to the 'up' page -->
	<div class="spacer">
	&nbsp;
	</div>
	<div class="rellink">
	<a href="../supervised_learning.html" title="3. Supervised learning" >
	Up
	<br>
	<span class="smallrellink">
	3. Supervised le...
	</span>
	<span class="hiddenrellink">
	3. Supervised learning
	</span>
	
	</a>
	</div>
    </div>
    <p style="text-align: center">This documentation is
    for scikit-learn <strong>version 0.11-git</strong>
    &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    
    <h3>Citing</h3>
    <p>If you use the software, please consider
    <a href="../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <h3>This page</h3>
	<ul>
<li><a class="reference internal" href="#">3.2. Support Vector Machines</a><ul>
<li><a class="reference internal" href="#classification">3.2.1. Classification</a><ul>
<li><a class="reference internal" href="#multi-class-classification">3.2.1.1. Multi-class classification</a></li>
<li><a class="reference internal" href="#unbalanced-problems">3.2.1.2. Unbalanced problems</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regression">3.2.2. Regression</a></li>
<li><a class="reference internal" href="#density-estimation-novelty-detection">3.2.3. Density estimation, novelty detection</a></li>
<li><a class="reference internal" href="#complexity">3.2.4. Complexity</a></li>
<li><a class="reference internal" href="#tips-on-practical-use">3.2.5. Tips on Practical Use</a></li>
<li><a class="reference internal" href="#kernel-functions">3.2.6. Kernel functions</a><ul>
<li><a class="reference internal" href="#custom-kernels">3.2.6.1. Custom Kernels</a><ul>
<li><a class="reference internal" href="#using-python-functions-as-kernels">3.2.6.1.1. Using python functions as kernels</a></li>
<li><a class="reference internal" href="#using-the-gram-matrix">3.2.6.1.2. Using the Gram matrix</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#mathematical-formulation">3.2.7. Mathematical formulation</a><ul>
<li><a class="reference internal" href="#svc">3.2.7.1. SVC</a></li>
<li><a class="reference internal" href="#nusvc">3.2.7.2. NuSVC</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-details">3.2.8. Implementation details</a></li>
</ul>
</li>
</ul>

    
    </div>

      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="support-vector-machines">
<span id="svm"></span><h1>3.2. Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">¶</a></h1>
<p><strong>Support vector machines (SVMs)</strong> are a set of supervised learning
methods used for <a class="reference internal" href="#svm-classification"><em>classification</em></a>,
<a class="reference internal" href="#svm-regression"><em>regression</em></a> and <a class="reference internal" href="#svm-outlier-detection"><em>outliers detection</em></a>.</p>
<p>The advantages of support vector machines are:</p>
<blockquote>
<div><ul class="simple">
<li>Effective in high dimensional spaces.</li>
<li>Still effective in cases where number of dimensions is greater
than the number of samples.</li>
<li>Uses a subset of training points in the decision function (called
support vectors), so it is also memory efficient.</li>
<li>Versatile: different <a class="reference internal" href="#svm-kernels"><em>Kernel functions</em></a> can be
specified for the decision function. Common kernels are
provided, but it is also possible to specify custom kernels.</li>
</ul>
</div></blockquote>
<p>The disadvantages of support vector machines include:</p>
<blockquote>
<div><ul class="simple">
<li>If the number of features is much greater than the number of
samples, the method is likely to give poor performances.</li>
<li>SVMs do not directly provide probability estimates, these are
calculated using five-fold cross-validation, and thus
performance can suffer.</li>
</ul>
</div></blockquote>
<p>The support vector machines in scikit-learn support both dens
(<tt class="docutils literal"><span class="pre">numpy.ndarray</span></tt> and convertible to that by <tt class="docutils literal"><span class="pre">numpy.asarray</span></tt>) and
sparse (any <tt class="docutils literal"><span class="pre">scipy.sparse</span></tt>) sample vectors as input. However, to use
an SVM to make predictions for sparse data, it must have been fit on such
data. For optimal performance, use C-ordered <tt class="docutils literal"><span class="pre">numpy.ndarray</span></tt> (dense) or
<tt class="docutils literal"><span class="pre">scipy.sparse.csr_matrix</span></tt> (sparse) with <tt class="docutils literal"><span class="pre">dtype=float64</span></tt>.</p>
<p>In previous versions of scikit-learn, sparse input support existed only
in the <tt class="docutils literal"><span class="pre">sklearn.svm.sparse</span></tt> module which duplicated the <tt class="docutils literal"><span class="pre">sklearn.svm</span></tt>
interface. This module still exists for backward compatibility, but is
deprecated and will be removed in scikit-learn 0.12.</p>
<div class="section" id="classification">
<span id="svm-classification"></span><h2>3.2.1. Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> are classes
capable of performing multi-class classification on a dataset.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_iris.html"><img alt="../_images/plot_iris_12.png" src="../_images/plot_iris_12.png" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> are similar methods, but accept
slightly different sets of parameters and have different mathematical
formulations (see section <a class="reference internal" href="#svm-mathematical-formulation"><em>Mathematical formulation</em></a>). On the
other hand, <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> is another implementation of Support
Vector Classification for the case of a linear kernel. Note that
<a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> does not accept keyword &#8216;kernel&#8217;, as this is
assumed to be linear. It also lacks some of the members of
<a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>, like support_.</p>
<p>As other classifiers, <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> take as input two arrays: an array X of size
[n_samples, n_features] holding the training samples, and an array Y
of integer values, size [n_samples], holding the class labels for the
training samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>  
<span class="go">SVC(C=None, cache_size=200, class_weight=None, coef0=0.0, degree=3,</span>
<span class="go">gamma=0.5, kernel=&#39;rbf&#39;, probability=False, scale_C=True, shrinking=True,</span>
<span class="go">tol=0.001)</span>
</pre></div>
</div>
<p>After being fitted, the model can then be used to predict new values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([ 1.])</span>
</pre></div>
</div>
<p>SVMs decision function depends on some subset of the training data,
called the support vectors. Some properties of these support vectors
can be found in members <cite>support_vectors_</cite>, <cite>support_</cite> and
<cite>n_support</cite>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># get support vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="go">array([[ 0.,  0.],</span>
<span class="go">       [ 1.,  1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># get indices of support vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([0, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># get number of support vectors for each class</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">n_support_</span> 
<span class="go">array([1, 1]...)</span>
</pre></div>
</div>
<div class="section" id="multi-class-classification">
<span id="svm-multi-class"></span><h3>3.2.1.1. Multi-class classification<a class="headerlink" href="#multi-class-classification" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> implement the &#8220;one-against-one&#8221;
approach (Knerr et al., 1990) for multi- class classification. If
n_class is the number of classes, then n_class * (n_class - 1)/2
classifiers are constructed and each one trains data from two classes:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> 
<span class="go">SVC(C=None, cache_size=200, class_weight=None, coef0=0.0, degree=3,</span>
<span class="go">gamma=1.0, kernel=&#39;rbf&#39;, probability=False, scale_C=True, shrinking=True,</span>
<span class="go">tol=0.001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c"># 4 classes: 4*3/2 = 6</span>
<span class="go">6</span>
</pre></div>
</div>
<p>On the other hand, <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> implements &#8220;one-vs-the-rest&#8221;
multi-class strategy, thus training n_class models. If there are only
two classes, only one model is trained:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lin_clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lin_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> 
<span class="go">LinearSVC(C=None, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="go">intercept_scaling=1, loss=&#39;l2&#39;, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;,</span>
<span class="go">scale_C=True, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">lin_clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">4</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#svm-mathematical-formulation"><em>Mathematical formulation</em></a> for a complete description of
the decision function.</p>
<p>Note that the <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> also implements an alternative multi-class
strategy, the so-called multi-class SVM formulated by Crammer and Singer, by
using the option &#8220;multi_class=&#8217;crammer_singer&#8217;&#8221;. This method is consistent,
which is not true for one-vs-rest classification.
In practice, on-vs-rest classification is usually preferred, since the results
are mostly similar, but the runtime is significantly less.</p>
<p>For &#8220;one-vs-rest&#8221; <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> the attributes <tt class="docutils literal"><span class="pre">coef_</span></tt> and <tt class="docutils literal"><span class="pre">intercept_</span></tt>
have the shape <tt class="docutils literal"><span class="pre">[n_class,</span> <span class="pre">n_features]</span></tt> and <tt class="docutils literal"><span class="pre">[n_class]</span></tt> respectively.
Each row of the coefficients corresponds to one of the <tt class="docutils literal"><span class="pre">n_class</span></tt> many
&#8220;one-vs-rest&#8221; classifiers and simliar for the interecepts, in the
order of the &#8220;one&#8221; class.</p>
<p>In the case of &#8220;one-vs-one&#8221; <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, the layout of the attributes
is a little more involved. In the case of having a linear kernel,
The layout of <tt class="docutils literal"><span class="pre">coef_</span></tt> and <tt class="docutils literal"><span class="pre">intercept_</span></tt> is similar to the one
described for <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> described above, except that
the shape of <tt class="docutils literal"><span class="pre">coef_</span></tt> is <tt class="docutils literal"><span class="pre">[n_class</span> <span class="pre">*</span> <span class="pre">(n_class</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></tt>,
corresponding to as many binary classifiers. The order for classes
0 to n is &#8220;0 vs 1&#8221;, &#8220;0 vs 2&#8221; , ... &#8220;0 vs n&#8221;, &#8220;1 vs 2&#8221;, &#8220;1 vs 3&#8221;, &#8220;1 vs n&#8221;, . .
. &#8220;n-1 vs n&#8221;.</p>
<p>The shape of <tt class="docutils literal"><span class="pre">dual_coef_</span></tt> is <tt class="docutils literal"><span class="pre">[n_class-1,</span> <span class="pre">n_SV]</span></tt> with
a somewhat hard to grasp layout.
The columns correspond to the support vectors involved in any
of the <tt class="docutils literal"><span class="pre">n_class</span> <span class="pre">*</span> <span class="pre">(n_class</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></tt> &#8220;one-vs-one&#8221; classifiers.
Each of the support vectors is used in <tt class="docutils literal"><span class="pre">n_class</span> <span class="pre">-</span> <span class="pre">1</span></tt> classifiers.
The <tt class="docutils literal"><span class="pre">n_class</span> <span class="pre">-</span> <span class="pre">1</span></tt> entries in each row correspond to the dual coefficients
for these classifiers.</p>
<p>This might be made more clear by an example:</p>
<blockquote>
<div><p>Consider a three class problem with with class 0 having 3 support vectors
<img class="math" src="../_images/math/421f60d3b5560d5dfa098fda140a1671e216c65f.png" alt="v^{0}_0, v^{1}_0, v^{2}_0"/> and class 1 and 2 having two support
vectors <img class="math" src="../_images/math/5e560f2b1854f96e2d455311e2593d5429ffa78e.png" alt="v^{0}_1, v^{1}_1"/> and <img class="math" src="../_images/math/5e560f2b1854f96e2d455311e2593d5429ffa78e.png" alt="v^{0}_1, v^{1}_1"/> respectively.
For each support vector <img class="math" src="../_images/math/c988973fead8b3040b3985509be86938fea6e7fa.png" alt="v^{j}_i"/>, there are 2 dual coefficients.
Let&#8217;s call the coefficient of support vector <img class="math" src="../_images/math/c988973fead8b3040b3985509be86938fea6e7fa.png" alt="v^{j}_i"/> in the
classifier between classes <cite>i</cite> and <cite>k</cite> <img class="math" src="../_images/math/8b03b125c6b3c91df87c02c7e7858426bd928c48.png" alt="\alpha^{j}_{i,k}"/>.
Then <tt class="docutils literal"><span class="pre">dual_coef_</span></tt> looks like this:</p>
<table border="1" class="docutils">
<colgroup>
<col width="36%" />
<col width="36%" />
<col width="27%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><img class="math" src="../_images/math/da127577a7daa5d7543fcaeabfa911a3babe7ed0.png" alt="\alpha^{0}_{0,1}"/></td>
<td><img class="math" src="../_images/math/42a9af7401c61b1641b6f941968ce24ec7595a13.png" alt="\alpha^{0}_{0,2}"/></td>
<td rowspan="3"><p class="first">Coefficients</p>
<p>for SVs</p>
<p class="last">of class 0</p>
</td>
</tr>
<tr class="row-even"><td><img class="math" src="../_images/math/fe0ef400d241f9cd7166d12066debb749204c269.png" alt="\alpha^{1}_{0,1}"/></td>
<td><img class="math" src="../_images/math/36488fb45eb71c1fc2dc7188deef64367e4790b3.png" alt="\alpha^{1}_{0,2}"/></td>
</tr>
<tr class="row-odd"><td><img class="math" src="../_images/math/1e4957af83ff0fc3313bf3a729d98cc13a26515e.png" alt="\alpha^{2}_{0,1}"/></td>
<td><img class="math" src="../_images/math/54e705927657eba7942fa5d78efcec4918ff960c.png" alt="\alpha^{2}_{0,2}"/></td>
</tr>
<tr class="row-even"><td><img class="math" src="../_images/math/d622b5bb4062b87b1aa043e83f739d1d508a6fdf.png" alt="\alpha^{0}_{1,0}"/></td>
<td><img class="math" src="../_images/math/1e70845f14f8393032f489280c42092132c732f0.png" alt="\alpha^{0}_{1,2}"/></td>
<td rowspan="2"><p class="first">Coefficients</p>
<p class="last">for SVs of class 1</p>
</td>
</tr>
<tr class="row-odd"><td><img class="math" src="../_images/math/c2a5a030027fb6cd12f62fe1c6f8f769de9a7f0a.png" alt="\alpha^{1}_{1,0}"/></td>
<td><img class="math" src="../_images/math/f269c8766a84c395bdede1b12f352079bd232366.png" alt="\alpha^{1}_{1,2}"/></td>
</tr>
<tr class="row-even"><td><img class="math" src="../_images/math/8413ee6f5048ca08c30c56562fe4d6330f18d64a.png" alt="\alpha^{0}_{2,0}"/></td>
<td><img class="math" src="../_images/math/d07311fe219a4465936ba186768e3f71c7ae48b0.png" alt="\alpha^{0}_{2,1}"/></td>
<td rowspan="2"><p class="first">Coefficients</p>
<p class="last">for SVs of class 2</p>
</td>
</tr>
<tr class="row-odd"><td><img class="math" src="../_images/math/5e92d7962655b23823c06bae96cb373d8dd47680.png" alt="\alpha^{1}_{2,0}"/></td>
<td><img class="math" src="../_images/math/22e8c1d955977e15d24672bbd38f619151eff837.png" alt="\alpha^{1}_{2,1}"/></td>
</tr>
</tbody>
</table>
</div></blockquote>
</div>
<div class="section" id="unbalanced-problems">
<h3>3.2.1.2. Unbalanced problems<a class="headerlink" href="#unbalanced-problems" title="Permalink to this headline">¶</a></h3>
<p>In problems where it is desired to give more importance to certain
classes or certain individual samples keywords <tt class="docutils literal"><span class="pre">class_weight</span></tt> and
<tt class="docutils literal"><span class="pre">sample_weight</span></tt> can be used.</p>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> (but not <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>) implement a keyword
<tt class="docutils literal"><span class="pre">class_weight</span></tt> in the fit method. It&#8217;s a dictionary of the form
<tt class="docutils literal"><span class="pre">{class_label</span> <span class="pre">:</span> <span class="pre">value}</span></tt>, where value is a floating point number &gt; 0
that sets the parameter C of class <tt class="docutils literal"><span class="pre">class_label</span></tt> to C * value.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html"><img alt="../_images/plot_separating_hyperplane_unbalanced_11.png" src="../_images/plot_separating_hyperplane_unbalanced_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><tt class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></tt></a> implement also weights for individual samples in method
<tt class="docutils literal"><span class="pre">fit</span></tt> through keyword sample_weight.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_weighted_samples.html"><img alt="../_images/plot_weighted_samples_11.png" src="../_images/plot_weighted_samples_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_iris.html#example-svm-plot-iris-py"><em>Plot different SVM classifiers in the iris dataset</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py"><em>SVM: Maximum margin separating hyperplane</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#example-svm-plot-separating-hyperplane-unbalanced-py"><em>SVM: Separating hyperplane for unbalanced classes</em></a></li>
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_anova.html#example-svm-plot-svm-anova-py"><em>SVM-Anova: SVM with univariate feature selection</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_nonlinear.html#example-svm-plot-svm-nonlinear-py"><em>Non-linear SVM</em></a></li>
<li><a class="reference internal" href="../auto_examples/svm/plot_weighted_samples.html#example-svm-plot-weighted-samples-py"><em>SVM: Weighted samples</em></a>,</li>
</ul>
</div>
</div>
</div>
<div class="section" id="regression">
<span id="svm-regression"></span><h2>3.2.2. Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<p>The method of Support Vector Classification can be extended to solve
regression problems. This method is called Support Vector Regression.</p>
<p>The model produced by support vector classification (as described
above) depends only on a subset of the training data, because the cost
function for building the model does not care about training points
that lie beyond the margin. Analogously, the model produced by Support
Vector Regression depends only on a subset of the training data,
because the cost function for building the model ignores any training
data close to the model prediction.</p>
<p>There are two flavors of Support Vector Regression: <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a>.</p>
<p>As with classification classes, the fit method will take as
argument vectors X, y, only that in this case y is expected to have
floating point values instead of integer values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVR</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVR(C=None, cache_size=200, coef0=0.0, degree=3,</span>
<span class="go">epsilon=0.1, gamma=0.5, kernel=&#39;rbf&#39;, probability=False, scale_C=True,</span>
<span class="go">shrinking=True, tol=0.001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([ 1.5])</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_regression.html#example-svm-plot-svm-regression-py"><em>Support Vector Regression (SVR) using linear and non-linear kernels</em></a></li>
</ul>
</div>
</div>
<div class="section" id="density-estimation-novelty-detection">
<span id="svm-outlier-detection"></span><h2>3.2.3. Density estimation, novelty detection<a class="headerlink" href="#density-estimation-novelty-detection" title="Permalink to this headline">¶</a></h2>
<p>One-class SVM is used for novelty detection, that is, given a set of
samples, it will detect the soft boundary of that set so as to
classify new points as belonging to that set or not. The class that
implements this is called <a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><tt class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></tt></a>.</p>
<p>In this case, as it is a type of unsupervised learning, the fit method
will only take as input an array X, as there are no class labels.</p>
<p>See, section <a class="reference internal" href="outlier_detection.html#outlier-detection"><em>Novelty and Outlier Detection</em></a> for more details on this usage.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_oneclass.html"><img alt="../_images/plot_oneclass_11.png" src="../_images/plot_oneclass_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_oneclass.html#example-svm-plot-oneclass-py"><em>One-class SVM with non-linear kernel (RBF)</em></a></li>
<li><a class="reference internal" href="../auto_examples/applications/plot_species_distribution_modeling.html#example-applications-plot-species-distribution-modeling-py"><em>Species distribution modeling</em></a></li>
</ul>
</div>
</div>
<div class="section" id="complexity">
<h2>3.2.4. Complexity<a class="headerlink" href="#complexity" title="Permalink to this headline">¶</a></h2>
<p>Support Vector Machines are powerful tools, but their compute and
storage requirements increase rapidly with the number of training
vectors. The core of an SVM is a quadratic programming problem (QP),
separating support vectors from the rest of the training data. The QP
solver used by this <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a>-based implementation scales between
<img class="math" src="../_images/math/1f1311c79b186e9a1e4aabcf5892dffc661c0264.png" alt="O(n_{features} \times n_{samples}^2)"/> and
<img class="math" src="../_images/math/7fa7e2716acbd8ca70826fdacdbb446261ee7f51.png" alt="O(n_{features} \times n_{samples}^3)"/> depending on how efficiently
the <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> cache is used in practice (dataset dependent). If the data
is very sparse <img class="math" src="../_images/math/02e494853b2b09e9c00e1c6b80864bc8b016bf5a.png" alt="n_{features}"/> should be replaced by the average number
of non-zero features in a sample vector.</p>
<p>Also note that for the linear case, the algorithm used in
<a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> by the <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> implementation is much more
efficient than its <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a>-based <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> counterpart and can
scale almost linearly to millions of samples and/or features.</p>
</div>
<div class="section" id="tips-on-practical-use">
<h2>3.2.5. Tips on Practical Use<a class="headerlink" href="#tips-on-practical-use" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul>
<li><p class="first"><strong>Avoiding data copy</strong>: For SVC, SVR, NuSVC and NuSVR, if the data
passed to certain methods is not C-ordered contiguous, and double
precision, it will be copied before calling the underlying C
implementation. You can check whether a give numpy array is
C-contiguous by inspecting its <cite>flags</cite> attribute.</p>
<p>For LinearSVC (and LogisticRegression) any input passed as a
numpy array will be copied and converted to the liblinear
internal sparse data representation (double precision floats
and int32 indices of non-zero components). If you want to fit
a large-scale linear classifier without copying a dense numpy
C-contiguous double precision array as input we suggest to use
the SGDClassifier class instead. The objective function can be
configured to be almost the same as the LinearSVC model.</p>
</li>
<li><p class="first"><strong>Kernel cache size</strong>: For SVC, SVR, nuSVC and NuSVR, the size of
the kernel cache has a strong impact on run times for larger
problems.  If you have enough RAM available, it is recommended to
set <cite>cache_size</cite> to a higher value than the default of 200(MB),
such as 500(MB) or 1000(MB).</p>
</li>
<li><p class="first"><strong>Setting C</strong>: In constrast to the scaling in LibSVM and LibLinear,
the <tt class="docutils literal"><span class="pre">C</span></tt> parameter in <cite>sklearn.svm</cite> is a per sample penalty.
Commonly good values for <tt class="docutils literal"><span class="pre">C</span></tt> often are very large (i.e. <tt class="docutils literal"><span class="pre">10**4</span></tt>)
and seldom below <tt class="docutils literal"><span class="pre">1</span></tt>.</p>
</li>
<li><p class="first">Support Vector Machine algorithms are not scale invariant, so <strong>it
is highly recommended to scale your data</strong>. For example, scale each
attribute on the input vector X to [0,1] or [-1,+1], or standardize it
to have mean 0 and variance 1. Note that the <em>same</em> scaling must be
applied to the test vector to obtain meaningful results. See section
<a class="reference internal" href="preprocessing.html#preprocessing"><em>Preprocessing data</em></a> for more details on scaling and normalization.</p>
</li>
<li><p class="first">Parameter nu in NuSVC/OneClassSVM/NuSVR approximates the fraction
of training errors and support vectors.</p>
</li>
<li><p class="first">In SVC, if data for classification are unbalanced (e.g. many
positive and few negative), set class_weight=&#8217;auto&#8217; and/or try
different penalty parameters C.</p>
</li>
<li><p class="first">The underlying <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> implementation uses a random
number generator to select features when fitting the model. It is
thus not uncommon, to have slightly different results for the same
input data. If that happens, try with a smaller tol parameter.</p>
</li>
<li><p class="first">Using L1 penalization as provided by LinearSVC(loss=&#8217;l2&#8217;,
penalty=&#8217;l1&#8217;, dual=False) yields a sparse solution, i.e. only a subset of
feature weights is different from zero and contribute to the decision
function.  Increasing C yields a more complex model (more feature are
selected).  The C value that yields a &#8220;null&#8221; model (all weights equal to
zero) can be calculated using <a class="reference internal" href="generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c" title="sklearn.svm.l1_min_c"><tt class="xref py py-func docutils literal"><span class="pre">l1_min_c</span></tt></a>.</p>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="kernel-functions">
<span id="svm-kernels"></span><h2>3.2.6. Kernel functions<a class="headerlink" href="#kernel-functions" title="Permalink to this headline">¶</a></h2>
<p>The <em>kernel function</em> can be any of the following:</p>
<blockquote>
<div><ul class="simple">
<li>linear: <img class="math" src="../_images/math/89236f7fdb8d5a6abe203db9593cab9b08b0b8fd.png" alt="&lt;x_i, x_j'&gt;"/>.</li>
<li>polynomial: <img class="math" src="../_images/math/8c848651b6dd9e1c081f50d8966208bcbacec38a.png" alt="(\gamma &lt;x, x'&gt; + r)^d"/>. <cite>d</cite> is specified by
keyword <tt class="docutils literal"><span class="pre">degree</span></tt>, <cite>r</cite> by <tt class="docutils literal"><span class="pre">coef0</span></tt>.</li>
<li>rbf (<img class="math" src="../_images/math/e5905fecf04a056f4580f94e9bc322ab2f593665.png" alt="exp(-\gamma |x-x'|^2), \gamma &gt; 0"/>). <img class="math" src="../_images/math/66981fa3920210c6ad8dbe5e968783d5dd7520c3.png" alt="\gamma"/> is
specified by keyword <tt class="docutils literal"><span class="pre">gamma</span></tt>.</li>
<li>sigmoid (<img class="math" src="../_images/math/a7a07268e39563daf2ade25aa1e0f279f6f6f458.png" alt="tanh(&lt;x_i,x_j&gt; + r)"/>), where <cite>r</cite> is specified by
<tt class="docutils literal"><span class="pre">coef0</span></tt>.</li>
</ul>
</div></blockquote>
<p>Different kernels are specified by keyword kernel at initialization:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">linear_svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear_svc</span><span class="o">.</span><span class="n">kernel</span>
<span class="go">&#39;linear&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rbf_svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;rbf&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rbf_svc</span><span class="o">.</span><span class="n">kernel</span>
<span class="go">&#39;rbf&#39;</span>
</pre></div>
</div>
<div class="section" id="custom-kernels">
<h3>3.2.6.1. Custom Kernels<a class="headerlink" href="#custom-kernels" title="Permalink to this headline">¶</a></h3>
<p>You can define your own kernels by either giving the kernel as a
python function or by precomputing the Gram matrix.</p>
<p>Classifiers with custom kernels behave the same way as any other
classifiers, except that:</p>
<blockquote>
<div><ul class="simple">
<li>Field <cite>support_vectors_</cite> is now empty, only indices of support
vectors are stored in <cite>support_</cite></li>
<li>A reference (and not a copy) of the first argument in the fit()
method is stored for future reference. If that array changes
between the use of fit() and predict() you will have unexpected
results.</li>
</ul>
</div></blockquote>
<div class="section" id="using-python-functions-as-kernels">
<h4>3.2.6.1.1. Using python functions as kernels<a class="headerlink" href="#using-python-functions-as-kernels" title="Permalink to this headline">¶</a></h4>
<p>You can also use your own defined kernels by passing a function to the
keyword <cite>kernel</cite> in the constructor.</p>
<p>Your kernel must take as arguments two matrices and return a third matrix.</p>
<p>The following code defines a linear kernel and creates a classifier
instance that will use that kernel:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">my_kernel</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_custom_kernel.html#example-svm-plot-custom-kernel-py"><em>SVM with custom kernel</em></a>.</li>
</ul>
</div>
</div>
<div class="section" id="using-the-gram-matrix">
<h4>3.2.6.1.2. Using the Gram matrix<a class="headerlink" href="#using-the-gram-matrix" title="Permalink to this headline">¶</a></h4>
<p>Set kernel=&#8217;precomputed&#8217; and pass the Gram matrix instead of X in the
fit method. At the moment, the kernel values between <cite>all</cite> training
vectors and the test vectors must be provided.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;precomputed&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># linear kernel computation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gram</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gram</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVC(C=None, cache_size=200, class_weight=None, coef0=0.0, degree=3,</span>
<span class="go">gamma=0.0, kernel=&#39;precomputed&#39;, probability=False, scale_C=True,</span>
<span class="go">shrinking=True, tol=0.001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># predict on training examples</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span>
<span class="go">array([ 0.,  1.])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="mathematical-formulation">
<span id="svm-mathematical-formulation"></span><h2>3.2.7. Mathematical formulation<a class="headerlink" href="#mathematical-formulation" title="Permalink to this headline">¶</a></h2>
<p>A support vector machine constructs a hyper-plane or set of hyper-planes
in a high or infinite dimensional space, which can be used for
classification, regression or other tasks. Intuitively, a good
separation is achieved by the hyper-plane that has the largest distance
to the nearest training data points of any class (so-called functional
margin), since in general the larger the margin the lower the
generalization error of the classifier.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_separating_hyperplane_11.png"><img alt="../_images/plot_separating_hyperplane_11.png" src="../_images/plot_separating_hyperplane_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="section" id="svc">
<h3>3.2.7.1. SVC<a class="headerlink" href="#svc" title="Permalink to this headline">¶</a></h3>
<p>Given training vectors <img class="math" src="../_images/math/a09ea178decd47aa7b07b21c6d97ef5347e2e602.png" alt="x_i \in R^p"/>, i=1,..., n, in two
classes, and a vector <img class="math" src="../_images/math/fc785dee08aa3f880f748fd19d636092154dd5f6.png" alt="y \in R^n"/> such that <img class="math" src="../_images/math/62d1091cef56573347ba66526e685759c228df94.png" alt="y_i \in {1,
-1}"/>, SVC solves the following primal problem:</p>
<div class="math">
<p><img src="../_images/math/b45838fb8b981db8fe3bf693bc4fac7fef62eb94.png" alt="\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1, n} \zeta_i



\textrm {subject to } &amp; y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
&amp; \zeta_i \geq 0, i=1, ..., n"/></p>
</div><p>Its dual is</p>
<div class="math">
<p><img src="../_images/math/2e17355bcd769a4b9d93acb2eccb9c3c4151a3c1.png" alt="\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha


\textrm {subject to } &amp; y^T \alpha = 0\\
&amp; 0 \leq \alpha_i \leq C, i=1, ..., l"/></p>
</div><p>where <img class="math" src="../_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> is the vector of all ones, C &gt; 0 is the upper bound, Q
is an n by n positive semidefinite matrix, <img class="math" src="../_images/math/20c63d702d950df0a5100366cf2227a3b0bb9e11.png" alt="Q_ij \equiv K(x_i,
x_j)"/> and <img class="math" src="../_images/math/2519dc33443f6b4bb0fd9cdc24ad7ba4e662ddea.png" alt="\phi (x_i)^T \phi (x)"/> is the kernel. Here training
vectors are mapped into a higher (maybe infinite) dimensional space by
the function <img class="math" src="../_images/math/2c175f60eecef1de7560c3bdea495d69f26f719d.png" alt="\phi"/>.</p>
<p>The decision function is:</p>
<div class="math">
<p><img src="../_images/math/5a7ffa6b50084336ade2c17a3b5cdda93b1feb75.png" alt="sgn(\sum_{i=1}^n y_i \alpha_i K(x_i, x) + \rho)"/></p>
</div><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In practice to have <img class="math" src="../_images/math/c3355896da590fc491a10150a50416687626d7cc.png" alt="C"/> independent of the number of samples <img class="math" src="../_images/math/174fadd07fd54c9afe288e96558c92e0c1da733a.png" alt="n"/>,
<img class="math" src="../_images/math/c3355896da590fc491a10150a50416687626d7cc.png" alt="C"/> is scaled by <img class="math" src="../_images/math/174fadd07fd54c9afe288e96558c92e0c1da733a.png" alt="n"/> (Replace <img class="math" src="../_images/math/c3355896da590fc491a10150a50416687626d7cc.png" alt="C"/> by <img class="math" src="../_images/math/c0c9c201a34d4e288cdf3078d670e7173c834cf9.png" alt="\frac{C}{n}"/> in the
equations above). It corresponds to the scale_C parameter which is True
by default in all estimators since version 0.11.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">While SVM models derived from libsvm and liblinear use <em>C</em> as regularization
parameter, most other estimators use <em>alpha</em>. The relation between both is
<img class="math" src="../_images/math/292465edde6015598d63e515af59d34772a25066.png" alt="C = \frac{1}{alpha}"/>.</p>
</div>
<p>This parameters can be accessed through the members <cite>dual_coef_</cite>
which holds the product <img class="math" src="../_images/math/91e0ab578768e5a3fdb5657751c33d9f167ef639.png" alt="y_i \alpha_i"/>, <cite>support_vectors_</cite> which
holds the support vectors, and <cite>intercept_</cite> which holds the independent
term <img class="math" src="../_images/math/55fa4f9455edd1c68ca06e7969c9c58d075ac409.png" alt="-\rho"/> :</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215">&#8220;Automatic Capacity Tuning of Very Large VC-dimension Classifiers&#8221;</a>
I Guyon, B Boser, V Vapnik - Advances in neural information
processing 1993,</li>
<li><a class="reference external" href="http://www.springerlink.com/content/k238jx04hm87j80g/">&#8220;Support-vector networks&#8221;</a>
C. Cortes, V. Vapnik, Machine Leaming, 20, 273-297 (1995)</li>
</ul>
</div>
</div>
<div class="section" id="nusvc">
<h3>3.2.7.2. NuSVC<a class="headerlink" href="#nusvc" title="Permalink to this headline">¶</a></h3>
<p>We introduce a new parameter <img class="math" src="../_images/math/d6a7ccf879c4a4fe694033606332cb83806db296.png" alt="\nu"/> which controls the number of
support vectors and training errors. The parameter <img class="math" src="../_images/math/948360fd417f20a48ae449aa26ede1baef981854.png" alt="\nu \in (0,
1]"/> is an upper bound on the fraction of training errors and a lower
bound of the fraction of support vectors.</p>
<p>It can be shown that the <cite>nu</cite>-SVC formulation is a reparametrization
of the <cite>C</cite>-SVC and therefore mathematically equivalent.</p>
</div>
</div>
<div class="section" id="implementation-details">
<h2>3.2.8. Implementation details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">¶</a></h2>
<p>Internally, we use <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> and <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> to handle all
computations. These libraries are wrapped using C and Cython.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<p>For a description of the implementation and details of the algorithms
used, please refer to</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: a library for Support Vector Machines</a></li>
<li><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR &#8211; A Library for Large Linear Classification</a></li>
</ul>
</div></blockquote>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010–2011, scikit-learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.2. Design by <a href="http://webylimonada.com">Web y Limonada</a>.
    <span style="padding-left: 5ex;">
    <a href="../_sources/modules/svm.txt"
	    rel="nofollow">Show this page source</a>
    </span>
    </div>
  </body>
</html>